\chapter{Results} 
\label{ch:opint-results}

This section reports the results of the cluster analysis described in \cref{sec:clustering}. 
The performance assessment is one of the trickiest parts when coming to unsupervised methods -- and clustering in particular --, and it has long been debated \cite{von2012clustering, guyon2009clustering}.
The most straightforward approach is to resort to the within sum of squared errors and average silhouette width similarity measures leveraged during the training phase (see WSSE and ASW in \cref{sec:clustering}).
However, these metrics treat the data as points in the space and measure the resulting shape of the formed clusters. 
Hence, they rely on the assumptions that \textit{i)} the ``correct" output shape is known, \textit{ii)} the similarity measure is informative about the desired morphology, and \textit{iii)} the numeric representation of the data points is consistent with the avowed geometry.
A general target for \textit{i)} is commonly established as producing compact agglomerations of points, possibly well separated among each other. 
% In our case, the family of k-means algorithms tends to produce spherical clusters.
% This is a generic assumption that may work in multiple scenarios, and it is conceptually grounded by the presumption that clusters should be groups of points homogeneously scattered around a center of mass.
% Furthermore, the ASW metric indeed captures the compactness 
In the case of transfer errors, it is unclear what a proper output shape should be, but the general approach seems reasonable. Furthermore, the ASW indeed captures the desired morphology, which provides a convenient tool for \textit{ii)}.
Nevertheless, the previous building blocks can be leveraged only as long as the numeric representation of error strings complies with the above schema. 
This, in turn, presupposes that we know what a correct representation should be and that the adopted language model can reproduce an accurate mapping of the raw data into such embedding.
Unfortunately, this element is unknown in advance and hardly explorable a posteriori unless recurring to human review and interpretation.
%, and exploring it a posteriori is cumbersome and requires human review and interpretation.
Furthermore, the above construction is solely based on geometric arguments, and the actual meaning of the data points, i.e. whether the clustered messages share the same content, is not taken into account.

In light of the above concerns, our work splits the evaluation of performances into two complementary phases.
First, a qualitative assessment explores the cluster contents and expresses the goodness of fit based on their interpretability, i.e. how messages of the same cluster resemble each other's meaning.
Second, a quantitative evaluation is addressed by cross-checking the clustering result against the GGUS reported incidents. In this way, a more direct measure of impact is given by reckoning the ability of our approach to mimic current operations.

