\chapter{Infrastructure management} \label{ch:opint}
The automation of infrastructure management and maintenance has become crucial in recent years. 
The increasingly large scale of modern data centers, and the adoption of distributed resources that necessitate the interaction of diverse hardware and software components, have made this task extremely complex. Consequently, traditional approaches to infrastructure management where manual human intervention is required have become impractical or even useless. 
For this reason, several strategies were proposed to support operational workflows in various ways \cite{opint2020, opint2022, decker2020powerquality, diotalevi2019elk, padolski}.
Although listing a precise taxonomy of alternative methods is hard -- since the boundaries between different classes are often blurred and the categories may overlap -- a first distinction can be established based on the analysis intent.
A common approach is to focus on \textit{anomaly detection}, where the objective is to spot anomalous behaviors that may entangle underpinning faults in the system. The detected anomalies are then reported to experienced operators for further investigations and fixes.
However, sometimes the malfunctions are too many to be inspected and solved singularly. Hence, an option is to rely on \textit{error categorization} to reduce the number of reports to check by grouping similar issues. This approach assumes that similar problems have similar solutions, therefore it is possible to address all the events of a group by inspecting only one (or a few) of them.
A more desirable yet more complicated target is \textit{root-cause analysis} \cite{sole2017survey}. In this case, the objective is to identify the origin of the problem directly, thus entirely automating the diagnosis phase.
In turn, these approaches can be further split into methods that seek just root causes -- what induced the issues -- or plain explanations -- why/how the faults were conceived.

Another essential distinction is based on the time of intervention with respect to a system failure. 
The simplest approach is \textit{reactive maintenance}, where the human intervention occurs after a failure is detected. In this case, the fault diagnosis is performed \textit{post-mortem} and, if effective, it helps identify the problems and speed up the restoration of good operating status. 
Nonetheless, the failures are not avoided and downtimes or denial of services are impossible to avert.
Some approaches try to intervene proactively to overcome this limitation, performing the so-called \textit{preventive maintenance}. In this case, the objective is to set an optimal schedule of periodic interventions to preserve high Quality of Service (QoS) and prevent faults directly.
However, completely eradicating failures requires frequent maintenance that is often unnecessary, which increases management costs.
An alternative strategy to limit these extra expenses is \textit{predictive maintenance}. The idea is to monitor the infrastructure on the fly (\textit{real-time} or \textit{online} analysis) and try to predict when an intervention is required. In this way, the inconveniences deriving from system downtimes are limited, and the costs imputable to unnecessary hardware replacement are cut.

Apart from the end goal and the time requirements of a given use case, a further distinction can be established based on the analyzed information.
Indeed, the choice of which strategy to pursue is bound by the available data or, vice-versa, it restricts the applicable techniques.
A first family of approaches leverages overall workloads -- e.g. number of running processes, hardware resources usage, network saturation -- as indicators of infrastructure health and monitor their trends over time. The deviations from normal operations are considered anomalies and trigger alerts to be investigated by experts \cite{paltenghi2021}.
A second class relies on \textit{event logs} as the primary way to register key runtime information. These reports record events happening during the execution of a system to provide an audit trail that can be helpful to understand the system activity and diagnose problems.
This information can be exploited in various forms.  
Some approaches focus on log activity summary statistics (e.g. number of printed lines) and try to disentangle nominal behaviors from suspect activity \cite{decker2020fuzzy, decker2020granular, minarini2020time}. 
Other alternatives use the log content instead, thus directly analyzing the textual information contained in the log files \cite{giommi2019predmaintenance}. These vary from traditional keyword searches -- e.g. ``kill", ``error", ``fail", ``exception" -- and heuristics \cite{tisbeni2019} to smarter tools based on deep learning language models.
The advantage of such procedures is that the textual information can aid system experts finding root causes and explanations which are harder to grasp from sheer workloads.
Both families mentioned above can be adopted for both online monitoring and offline debugging, depending on the use case.
Finally, another class of techniques focuses on \textbf{error messages} and is only devoted to diagnosing issues in post-mortem analyses. Like event logs, errors can be exploited to conduct thorough root-cause investigations or just error categorization.
Although logs and error messages both collect textual data, their difference lies in the format this information is presented in and the actual content. 
Error messages only include system printouts related to failure events, while logs typically gather the full runtime information. 
In terms of format, error messages are cleaner compared to log files, despite both can be listed as semi-structured data. 
In fact, both classes can be summarized into a free-form constant string describing the status of a system, plus some parameters that record important system attributes.

This work tackles the problem of error categorization for post-mortem diagnosis of issues during WLCG data transfers.

