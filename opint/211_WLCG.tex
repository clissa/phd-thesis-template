\section{Background}
\begin{figure}
    \centering
    \subfloat[LHC accelerator]{
    \includegraphics[width=\textwidth]{figures/220_introduction/cern/a0lhcinternotubo_445034351.jpg}
    }
    
    \subfloat[Aerial view]{
    \includegraphics[width=0.5\textwidth]{figures/220_introduction/cern/CERN_location.jpg}
    }
    \subfloat[LHC scheme]{
    \includegraphics[width=0.5\textwidth]{figures/220_introduction/cern/lhc_scheme_factsfigures.png}
    }
    \caption{\textbf{LHC accelerator complex.} Top: the underground tunnel that hosts LHC and a transversal section of the pipes that compose it. 
    Bottom: aerial view of LHC complex at the boundary between Switzerland and France (left) and structure of the various accelerating structures that compose LHC (right).
    }
    \label{fig:lhc}
\end{figure}

\begin{figure}
    \centering
    \subfloat[ATLAS]{
    \includegraphics[width=0.5\textwidth]{figures/220_introduction/cern/atlas_detector_medium.jpg}
    }
    \subfloat[LHCb]{
    \includegraphics[width=0.5\textwidth]{figures/220_introduction/cern/lhcb_detector_medium.jpg}
    }
    
    \subfloat[ALICE]{
    \includegraphics[width=0.5\textwidth]{figures/220_introduction/cern/alice_detector_medium1.jpg}
    }
    \subfloat[CMS]{
    \includegraphics[width=0.5\textwidth]{figures/220_introduction/cern/cms_detector_medium.jpg}
    }
    \caption{\textbf{CERN 4 major experiments.}}
    \label{fig:cern_experiments}
\end{figure}

\note[Luca][notesyellow]{Introduction to HEP community, with mention to WLCG as an essential tool to perform their research}

% High-Energy Physics (HEP) is a branch of physics that studies the fundamental constituents of matter and the forces that drive their interactions. One of the methods is to create very high energy densities. 
% This reproduces the environmental conditions of the primordial universe.

High-Energy Physics (HEP) is a branch of physics that studies the elementary constituents of matter and the fundamental principles that govern their interaction to understand how our universe has formed and evolved.  
These particles, however, are not visible at the scales whereby we experience reality today. 
Thus, HEP experiments need to either look at natural phenomena generated in pressure and temperature conditions similar to those of the primordial universe -- like cosmic rays -- or recreate such settings artificially.

The European Council for Nuclear Research (CERN) is part of this second strand of experiments, and it constitutes the largest particle physics laboratory in the world.
From 2008, CERN facilities also include the Large Hadron Collider (LHC), the longest particle accelerator ever built.
LHC consists of a \mbox{26.7-kilometer} ring located in a tunnel about 100 meters underground in the Geneva area (\cref{fig:lhc}), and it is made of superconducting magnets with several accelerating structures \cite{lhcwebsite}.
Inside the accelerator, bunches of protons are revved up to nearly the speed of light, forming two high-energy particle beams that travel in opposite directions inside separated pipes. 
When they acquire the desired energy, the beams are directed towards dedicated interaction points where the experiments occur. %surrounded by giant detectors.
In practice, LHC hosts four major experiments built in correspondence of these locations -- ATLAS \cite{aad2008atlas}, ALICE \cite{aamodt2008alice}, LHCb \cite{alves2008lhcb} and CMS \cite{collaboration2008cms} -- and equipped with giant detectors (\cref{fig:cern_experiments}).
Once the beams get there, the two pipes cross and the particles are squeezed through substantial magnetic fields to increase their chances of colliding. 
In this way, a massive amount of energy is concentrated in an extremely tiny area, generating millions of particles at each collision.
Indeed, the high speed of the beams causes roughly 40 million crossings per second at each interaction point \cite{albrecht2019roadmap, grandi2017HEPsize}. 
\sidenote[Luca][notesyellow]{Controllare numeriche ed aggiungere reference}
When a crossing happens, an average of 60 bunch collisions -- also referred to as pileup -- are observed \cite{albrecht2019roadmap}. The particles produced by each scattering then fly around the interaction point to be eventually detected through high-technology experimental devices endowed with over 100 million electronic channels \cite{grandi2017HEPsize, aad2020channels}.
According to the latest experimental setup, this delivers 100 MegaBytes (MB) of data per collision and it would generate 40k ExaBytes (EB) every year \cite{grandi2017HEPsize}.
However, storing such a tremendous amount of data is unattainable with current technology and budget. In addition, the events of interest are typically rare, so there is actually no need to record all of the information detected by the electronic channels.
Thus, the vast majority of \emph{read-out} data from collisions is discarded straight away and the \emph{recorded} event rate is lowered to 1k crossings per second. 
As a result of this reduction, the actual acquisition rate
% amounts to 1MB every second, translating to roughly 100 PetaBytes (PB) a year in 2018 \cite{grandi2017HEPsize, altre?}.
amounts to nearly 1 PB per day \cite{cern2017storage}, translating to roughly 160 PetaBytes (PB)%
\footnote{LHC registered 161 days of physics data taking in 2018 \cite{todd2018lhcAvail}}
a year in 2018. %\cite{grandi2017HEPsize, cern2017datayear}.
Besides that, physics analyses require comparing experimental results with Monte Carlo data simulated according to current theories, thus producing somewhat between 1 and 2 times additional data \cite{grandi2017HEPsize}.
Furthermore, the CERN community is already working at enhancing the Large Hadron Collider capabilities.
The project involves boosting the energy of the beam and gradually increasing the pileup towards 200 collisions per bunch crossing \cite{albrecht2019roadmap}, thus leading to the so-called High Luminosity LHC (HL-LHC) \cite{hllhc}.
% Thanks to this upgrade, way more events will be observed as the beam energy will be boosted and the pileup will gradually be increased towards 150 collisions per bunch crossing.
% In this new regime, 
Thanks to this upgrade, the observed events are expected to increase of a factor $\geq5$\cite{giuspe2021tesi} and produce an estimated 800 PB of new data each year by 2026.%\cite{grandi2017HEPsize, HLLHC_data}.

Although it is difficult to replicate such a punctual measurement of the data production for other big data players, some hints can be retrieved by comparing multiple online resources. %\cite{BDPlot_series}. 
\Cref{fig:bidata_size} tries to summarize a reasonable, up-to-date ``guesstimate"%
\footnote{These data are reconstructed based on multiple online sources about the amount of contents produced, streamed or hosted by big data companies and reasonable estimates of unitary sizes for such contents, e.g. average mail or picture size, average data traffic for 1 hour video, and so on. 
However, the actual values reported are not meant to be extremely accurate and only serve the purpose of giving an idea of the orders of magnitude of the various phenomena.
}
of yearly data production for the main big data companies.
Despite not being the most popular among the mainstream audience, the HEP community is one of the most prominent players concerning big data.
% \begin{landscape}
% \begin{figure}
%     \centering
% \begin{tikzpicture}%[shorten >=1pt,node distance=1cm,on grid,auto] 
% \linespread{.5}%
% \begin{scope}
%     \node[anchor=south west,inner sep=0] (image) at (0,0){\includegraphics[width=\linewidth]{figures/220_introduction/BigData2021.pdf}};
% \begin{scope}[x={(image.south east)},y={(image.north west)}]
%     \node [anchor=west] (note1) at (0.26,0.170) {\href{https://stackoverflow.com}{Link}};
%         \node [anchor=west, scale=1, align=center,font=\tiny] (note2) at (0.26,0.470) {\href{https://rdcu.be/cB1Ds}{paper} \\ from the future};
% \end{scope}
%     \end{scope}
% \end{tikzpicture}
% \caption{\textbf{Big Data sizes.} Bubble plot of the orders of magnitude of data produced by important big data players. The balloon areas illustrate the amount of data and the text annotations highlight the key factors considered in the estimates. Average per-unit-sizes are reported in parentheses, where italic indicates measures reconstructed based on likely assumptions because no references were found.} \label{fig:bidata_size1}
% \end{figure}
% \end{landscape}
\begin{landscape}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/220_introduction/BigData2021.pdf}
    \caption{\textbf{Big Data sizes.} Bubble plot of the orders of magnitude of data produced by important big data players. The balloon areas illustrate the amount of data and the text annotations highlight the key factors considered in the estimates. Average per-unit-sizes are reported in parentheses, where italic indicates measures reconstructed based on likely assumptions because no references were found. Interactive version available at: \href{https://clissa.github.io/BigData2021/BigData2021.html}{BigData2021.html}}
    \label{fig:bidata_size}
\end{figure}
\end{landscape}

Indeed the read-out data LHC produced every year in Phase 2 (40k EB) is around one order of magnitude bigger than the total size of objects ever stored on Amazon AWS cloud service (500 EB)%
\footnote{Obtained considering the total number of objects reportedly stored in Amazon S3 (100 trillion, \citeA{amazon2021objectscount}) and assuming an average size of 5 MB based on some average bucket example \cite{amazon2021objectssize}}.
Considering effectively recorded data, LHC figures are comparable with those of other most renowned big data entities. The last run (2018), in fact, produced hundreds of PetaBytes (PB) (roughly 160 of real data and 240 of Monte Carlo simulations),
 which is similar to the orders of magnitude generated by Google searches (62 PB)%
\footnote{Obtained considering that Google search index contains at least 30 billion webpages \cite{van2016estimating, google2021index_size, djuraskovic2020googl_stats, indig2020index_size} and that the average page size is 2.07 MB \cite{http2021webpage_size}
},
Instagram and Facebook shared photos (68 and 252 PB, respectively)%
\footnote{Obtained considering that 65k and 240k pictures are shared every minute on Instagram and Facebook \cite{domo2021infographic}, and assuming 2 MB as a reasonable average picture size \cite{adobe2021fb_img_size}
}
and YouTube video uploads (263 PB)%
\footnote{Obtained considering that 720k hours of video are uploaded daily \cite{domo2021infographic} and assuming an average size of 1 GB \cite{quora2021youtube}
}.
Moreover, LHC will climb the table even further with the upgrade to high luminosity, when the real and Monte Carlo data production rate is expected to rise to levels comparable to those of storage services like Dropbox (800, 1200 and 768 PB%
\footnote{Obtained considering that Dropbox registered 100 million of new users in 2020, of which 1.17 million were paid subscriptions \cite{dean2021dropbox}. For the average per-unit-size, it was assumed that free accounts exploited 75\% of the 2 GB storage available, while paid ones exploited 25\% of the total 2 TB
}, respectively). 

Apart from the nominal values of the generated information, streaming data comprise a significant slice of the big data market.
As a matter of fact, the continual movement of small- to medium-sized files spawns massive traffic when scaled up to millions of users, as testified by e-mails (5.7k PB)%
\footnote{Obtained considering that 71k billion e-mails and 60k billion spam messages were sent from October 2020 to September 2021 \cite{statista2021mails}, and that the average size is \mbox{75 KB} for e-mails \cite{lifewire2021avg_mail} and \mbox{5} KB for spam \cite{medium2014avg_spam}
},
and Netflix (51.1k PB)%
\footnote{Obtained considering that Netflix users consumed 140 million hours per day of streaming \cite{domo2021infographic} and that counts for 1 GB of data for standard definition videos \cite{perry2021netflix}
} bubbles in \cref{fig:bidata_size}.
% Also in this respect, LHC plays an important role. Indeed, the HEP community is formed by thousands of researchers spread around the world that need to access the data produced at CERN.
A similar usage is generated also by the LHC, whose data are continuously transferred across the HEP community thanks to the Worldwide LHC Computing Grid (see \cref{wlcg}) to fuel innovative research. For example, 
% 12 PB of data were accessed on average every day in 2020 for the ATLAS experiment alone \cite{calafiura2020design_report}, and
a throughput of 60 GB/s was generated by the 4 experiments together in 2018 \cite{wlcg2018throughput} thus giving a yearly projection (1.9k PB) close to half of the global e-mails traffic and only one order of magnitude lower than Netflix usage.
% Indeed, thousands of researchers from all around the world need to access LHC data, analyze them and share their results to investigate new theories and advance our knowledge of the universe.
% For this reason, the Worldwide LHC Computing Grid computing infrastructure has been developed over the years 

Given the large quantities of data involved by the LHC, it is not surprising how careful planning must be done in order to meet the needs of the LHC community, and tailored strategies and technologies must be adopted to cope with such requirements. 
Luckily, the presence of other stakeholders facing analogous problems provides the HEP community with some alternatives to draw from, and it allows researchers to tap in from existing solutions and customize them for their necessities.

\subsection{The Worldwide LHC Computing Grid}
\label{wlcg}

\sidenote[Luca][notesyellow]{Cos'è WLCG? Qual è il suo scopo?}
The Worldwide LHC Computing Grid (WLCG) \cite{bird2011computing} is a global collaboration that links up more than 170 computing centers in 42 countries, serving an audience of more than 12000 physicists all around the world. 
As of 2022, WLCG constitutes the largest computing grid in the world and it is supported by many associated national and international grids, such as the European Grid Initiative and the Open Science Grid, as well as many other regional grids.
Founded in 2002 by CERN, the WLCG mission is to provide computing resources to store, distribute and analyze the data generated by the Large Hadron Collider.

\sidenote[Luca][notesyellow]{com'è fatta WLCG?}
Given the scale and complexity of the LHC data, this requires massive storage facilities, immense computing power, global networking, tailored software, adequate personpower and, of course, funding.
In order to achieve such challenging goals, WLCG leverages a distributed computing paradigm, where resources are shared among member states and made equally available to all the partners, regardless of their physical location.
% In brief, WLCG guarantees a seamless access to computing resources which include data storage capacity, processing power, sensors, and visualization tools, the resources that are capable to process over two million tasks daily, leveraging over one million computer cores and 1 exabyte of storage \cite{opint2022}.
\sidenote[Luca][notesyellow]{tiered structure?}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/220_introduction/WLCG-Tiers-2021_v3.png}
    \caption{\textbf{WLCG structure.} The Worldwide Large Hadron Collider Grid has a tiered structure organized into 4 levels and comprising more than 170 computing centers spread across 42 countries.
    } \label{fig:wlcg}
\end{figure}
\cref{fig:wlcg} summarizes the WLCG infastructure composition. 
The Worldwide Large Hadron Collider Grid has a hierarchical structure of 4 levels, called \textit{tiers}, differing in terms of computing resources, storage capabilities and delivered services. 
% Proceeding from the bottom of this architecture, each layer sees an increased number
Its bottom layers comprise a few computing centers having great amounts of storage and processing resources, ultra-fast network connectivity (up to 100 GB/s), and they are devoted to general processing tasks.
The shallower layers, instead, group many smaller data centers devoted to more specialized activities.
In particular, the CERN data center is located at the bottom of this infrastructure, constituting the cornerstone of the whole architecture. 
It is located in Geneve (Switzerland) and it is endowed with more than 73000 processor cores, providing around 20\% of the total compute capacity of WLCG.
In terms of activity, the Tier-0 is responsible for i) the management of the raw data streams coming from the LHC experiments and their archiving for safe-keeping, ii) the reconstruction of physical entities like particles energy and velocity\sidenote[Luca][notesyellow]{energy e velocity sono corrette?} starting from the raw read-outs recorded by the electronic equipment, and iii) the distribution of raw and reconstructed data to the next tier layers.
Moving up the WLCG architecture, we find 13 large computer centres of the \textit{Tier-1s}.
These are directly linked to the Tier-0 and contribute to WLCG operations with sufficient storage capacity and round-the-clock support for the users. They are responsible for i) the safe-keeping of a proportional share of raw and reconstructed data, ii) large-scale reprocessing and safe-keeping of corresponding output, iii) access and distribution of data to the next infrastructure levels, and iv) safe-keeping of Monte Carlo simulated data.
One of these Tier-1 sites is located in Bologna\footnote{\cnaf} and it represents the biggest data center in Italy.
The subsequent layer involves around 160 \textit{Tier-2} sites. These are data centers offered by universities and other scientific institutes, which are connected to some of the Tier-1 sites through regional networks. 
They essentially act as analysis facilities to perform specialized tasks as the experiments production jobs and the Monte Carlo simulated data. 
Despite having storage resources, the Tier-2s are not required to archive data, so the outputs produced there are typically sent back to Tier-1s when they are intended to be shared (as in the case of Monte Carlo data).
Finally, the last rung of the ladder is constituted by \textit{Tier-3s}. These sites enormously vary in scale,  ranging from local computing resources like university clusters or even individual pc to large national analysis facilities.
In fact, they do not formally belong to the WLCG but solely serve as entry points to its infrastructure for end-users analyses.

Alongside the hardware facilities, the WLCG supplies also advanced software solutions to provide researchers with seamless access to resources in a transparent way, without needing to worry about where the computing resources are coming from or where the data are physically stored.
This means that users can request access to data or resources from one of the many entry points into the system, and the grid infrastructure will then take care of spawning all the needed processes under the hood.
This may entail establishing the user identity and its access rights to the various sources, checking their credentials, and searching for available sites that can provide the requested resources.