\subsection{Related works}

\note[Luca][notesyellow]{da strutturare per bene, per il momento Ã¨ solo un copia e incolla da Frontiers OpInt}

The analysis of error messages encountered in large-scale distributed systems has become one of the crucial tasks for monitoring computing resources. 
Thus, a variety of tools for log and error message parsing has already been explored. 
These include frequent pattern mining, machine learning clustering, grouping by longest common subsequence, heuristics, parsing trees, and evolutionary algorithms~\cite{zhu2019tools}.
However, the existing tools have some drawbacks. First, most methods require a crucial pre-processing phase that may need deep customization for specific data. This limitation hampers their adaptation to novel use cases as different systems may have diverse logging conventions, terminology and structure.
Furthermore, they do not allow error messages to be linked with additional entities other than the textual information, meaning that messages cannot be clustered along with auxiliary data.

These approaches are typically made of two stages: \textit{i)} text vectorization and \textit{ii)} clustering.
The first step is needed to transform the textual information into a convenient numeric representation.
Once that is achieved, the resulting data are grouped thanks to clustering algorithms of various kinds. 

\citeA{clusterlog2021} presents a pipeline consisting of several stages specifically tailored for WLCG analysis workflows. 
First, the error messages are tokenized and cleaned from digit, punctuation and special characters.
Then, an hashing algorithm is used to replace the message parametric parts with a placeholder, and the resulting patterns are exploited for the next elaborations. In this way, the data variability is lowered and their total amount is reduced by 90-95\%.
After the above pre-processing, the vectorization stage is based on \textit{word2vec} \cite{word2vec} that computes a numerical representation of words. The overall message representation is then retrieved from the latter by simply averaging over single words embeddings. 
The resulting embedding is then reduced in dimension by means of PCA \cite{pca}, and a DBSCAN \cite{dbscan} algorithm is adopted for the clustering stage.
Finally, clusters description are extracted by searching common textual patterns and key phrases for all messages belonging to the same cluster.

\begin{enumerate}
    \item many custom choices: depend on use-case, thus difficult to adopt
    \item hard-coded feature engineer is heavy and hampers more granularity if needed
    \item suggested settings for dbscan produce hundreds of clusters --> impractical to check them all
    \item probably reduces to unique strings + key-phrase extraction
\end{enumerate}


