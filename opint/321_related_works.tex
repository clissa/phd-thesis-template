\subsection{Related works} \label{sec:related_opint}

\note[Luca][notesyellow]{da strutturare per bene, per il momento è solo un copia e incolla da Frontiers OpInt}

Analyzing error messages encountered in large-scale distributed systems has become one of the crucial tasks for monitoring computing resources. 
Thus, a variety of tools for log and error message parsing has already been explored. 
Some of these approaches include longest common subsequence \cite{du2016spell}, frequent pattern mining \cite{vaarandi2003slct, vaarandi2015logcluster}, iterative partitioning \cite{makanju2009iplom}, parsing trees \cite{he2017drain} and  hierarchical clustering \cite{fu2009lke} (see \citeNP{zhu2019tools} for a thorough discussion and comparison).
However, the existing tools present some drawbacks. First, most methods require a crucial pre-processing phase that may need deep customization for specific data. This limitation hampers their adaptation to novel use cases as different systems may have diverse logging conventions, terminology and structure.
Furthermore, they do not allow error messages to be linked with additional entities other than the textual information, meaning that messages cannot be clustered along with auxiliary data.

% These approaches are typically made of two stages: \textit{i)} text vectorization and \textit{ii)} clustering.
% The first step is needed to transform the textual information into a convenient numeric representation.
% Once that is achieved, the resulting data are grouped thanks to clustering algorithms of various kinds. 

\citeA{clusterlog2021} presents a pipeline consisting of several stages specifically tailored for WLCG analysis workflows. 
First, the error messages are tokenized and cleaned from digits, punctuation and special characters.
Then, a hashing algorithm replaces the parametric parts of the message with a placeholder, and the resulting patterns are exploited for the following elaborations. In this way, the total amount of data is reduced by 90-95\%.
After the above pre-processing, the vectorization stage is based on \textit{word2vec} \cite{mikolov2013word2vec} that computes a numerical representation for each token. The overall message representation is then retrieved by averaging over single word embeddings. \sidenote[Luca][notesyellow]{inserire reference a sezione successiva per spiegazione più approfondita}
The resulting representation is then reduced in dimension by means of principal components analysis \cite{wold1987pca}, and a DBSCAN \cite{ester1996dbscan} algorithm is adopted for the clustering stage.
Finally, cluster descriptions are extracted by searching common textual patterns and key phrases for all messages belonging to the same cluster.

Another interesting approach is presented in \citeA{lin2016log}, where the authors propose a convenient pipeline to group logs of failed jobs and exploit the knowledge coming from previous failures.
After substituting placeholders instead of parametric parts in the raw messages, each log is summarized using the unordered set of the events (log lines) it contains.
A vectorization stage is then performed based on Inverse-Document event Frequency (IDF) and contrast-based weighting. 
The resulting numerical representation undergoes an agglomerative hierarchical clustering algorithm that finds groups of similar logs.
The resulting cluster centroids are then taken as representative log sequences of their respective groups, and they are compared to a knowledge base of previous failures and corresponding solutions. If the sequence similarity to one of the known issues is above a given threshold, the corresponding actions are applied to solve the problem. Otherwise, the log sequence is passed to system experts for manual inspection and the reference dataset is successively updated.
In this way, human resources are involved only in handling new issues, while previous knowledge is exploited for recurrent ones.

Another way to look at this problem is through the lenses of Natural Language Processing (NLP), where related tasks have been addressed by adopting various strategies.
A direct approach would be to regard error categorization as a specific example of topic modeling \cite{hofmann1999probabilistic, papadimitriou2000latent}.
In brief, topic modeling resorts to a low-dimensional latent representation of textual data where each latent dimension may be interpreted as a separate topic.
In the context of error categorization, the different topics can be seen as high-level descriptions of different failures, and the messages as particular instances of the related problems.
Alternatively, popular \textit{language models} can also be leveraged \cite{devlin2018bert, peters2018elmo, radford2019gpt2}.
They consist of numeric representations for textual information -- also known as \textit{embeddings} -- that preserve syntactic, grammatical and semantic relations of the original data, but in a lower dimension. 
This means that words similar in terms of meaning and usage are projected near to each other.
Therefore, these techniques can be adopted to get convenient error embeddings where related failures are close in the sense of some distance or similarity measure, and clustering algorithms can be exploited to retrieve error categories.

