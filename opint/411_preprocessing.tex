\subsection{Pre-processing}

% Of course, the string format of the raw data is highly unstructured and impractical to handle, thus limiting the plethora of applicable techniques. 
% For this reason, the (possibly long) strings incorporated in the documents are first quantized into unitary pieces of textual information, tokens, from which the raw strings can be reconstructed. These process may vary from simply using words \cite{bengio2003word, mccann2017word} or characters \cite{ling2015char, dhingra2016char}, to more complex strategies involving subwords \cite{gage1994subword, sennrich2016subword}, sentences \cite{kiros2015sentence}, documents \cite{le2014documents} and topics \cite{niu2015topic}. 

The pre-processing phase is crucial to any data analysis workflow. Various best practices are suggested for data cleaning, and custom feature engineering is often adopted to feed models with the most relevant information in the most suitable format.
Our approach tries to limit these elaborations to the bare minimum to avoid injecting too much information into the system and probe the model's ability to learn by itself.
The resulting pipeline is described below and summarized in table 1.

As a first step, the raw error strings are enriched by appending the source and destination hostnames. In particular, both hostnames are inserted at the end of each message with prepended src\_ or dst\_ prefixes to distinguish whether they were involved as source or destination, respectively.
The resulting text then undergoes a process of quantization whereby the raw strings are decomposed into unitary pieces of information.
This process is commonly referred to as tokenization and the resulting atomic units are called tokens. Various approaches have been proposed in the literature ranging from simply using words \cite{bengio2003word, mccann2017word} or characters \cite{ling2015char, dhingra2016char}, to more complex strategies involving subwords \cite{gage1994subword, sennrich2016subword}, sentences \cite{kiros2015sentence}, documents \cite{le2014documents} and topics \cite{niu2015topic}. 
In our case, we resort to whitespace tokenization for the sake of simplicity, which means individual words are used as tokens.
Once tokens are obtained, they are stripped of leading and trailing punctuation (":;,.- ").
After that, tokens corresponding to common English stopwords or unuseful punctuation ["", ":", "-", "+"] are discarded.
Finally, the URL addresses are split into two components: the net location and the relative path of the requested resources. For instance, httpg://tbn18.nikhef.nl:8446/srm/managerv2 is decomposed as httpg://tbn18.nikhef.nl:8446 + /srm/managerv2 .
In this way, it is possible to exploit the compositional structure of URL addresses to reduce the vocabulary of unique tokens. Also, this allows the model to disentangle the contribution of single parts in different messages.