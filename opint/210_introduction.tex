
\chapter{Introduction}

We are witnessing an ever-increasing trend in data production, thus leading to the so-called \emph{Big Data} era. 
\note[Luca][notesyellow]{Espandere descrizione con esempi e brevi cenni storici}

In order to cope with the growing amount of data to store and process, the big data players of both industry and academy have gradually moved to new computing paradigms in recent years. 
For instance, new solutions as \textbf{distributed} and \textbf{cloud computing} \sidenote[Luca][notesyellow]{Spiegare pi√π approfonditamente?} have been specifically designed to address these new requirements, taking advantage of multiple resources geographically displaced and accessible via a network.

However, the boost in performance guaranteed by these technologies comes with the price of requiring very complex interactions of both hardware and software components. 
Aside from the enormous benefits these solutions bring, their most relevant drawback is that the wider the infrastructure, the higher the chances of something going wrong, and the bigger the effort to detect, inspect and solve the issues.
\Cref{partII} explores this domain and tries to propose a data-driven pipeline to ease and support people working to maintain the infrastructure integrity.
Despite applying to many different applications with some tuning, the presented approach is discussed in the context of \textbf{data transfer failures} within the \emph{Worldwide Large Hadron Collider Computing Grid} (WLCG).

\input{opint/211_WLCG}