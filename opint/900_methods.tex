\chapter{Methods}

This section presents a detailed description of an approach for dealing with File Transfer Service failures. % \cref{sec:FTS_failures}.
The main goal is to analyze FTS failed transfers and identify error categories as suggestions of potential issues to investigate further by human experts.
In terms of desiderata, the objective is to develop a tool independent of experiment-specific configurations and workflows. Hence, we focus on errors produced by FTS rather than other services which are adopted only by some collaborations, e.g. Rucio for ATLAS.
Likewise, a minimal experts' effort is required. For this reason, we embrace an unsupervised learning approach to force the model to learn autonomously from data without needing a costly labeling phase of previous failures.
As a byproduct, this strategy also avoids incurring expectation bias from prior (perhaps suboptimal) operative categorizations, and it enables discovering new failure patterns.

The proposed approach \cite[Section 2.19]{opint2022} is inspired by the pipeline described in \citeA{lin2016log}. %although only part of it has been developed since no knowledge base is available for our use case.
In particular, we adopt a 3-step workflow consisting of \textit{i)} vectorization, \textit{ii)} clustering and \textit{iii)} description stages. 
The last step of the original pipeline, i.e. checking recurrence, is excluded since no knowledge base is available for FTS failures.
Also, our work is conceptually similar to the strategy described in \citeA{clusterlog2021}. However, some major differences are present in the pre-processing, clustering and description stages%
, and they are discussed in detail in \cref{sec:pipeline}.
% to mitigate the limitations discussed in \cref{sec:related_opint}.

In practice, our pipeline is developed inside the Operational Intelligence software framework\footnote{\githubpyspark}, and it is implemented trying to cope with the runtime restrictions for online processing despite not renouncing model performance.
To achieve that, the \textit{Spark} \cite{zaharia2010spark} processing framework is used through the \texttt{pyspark} language to leverage the advantages of distributed computations for large data. 
In particular, the whole pipeline is executed through the \textbf{SWAN service} \cite{piparo2018swan} that allows CERN users to run \texttt{Jupyter Notebooks} connected to Spark clusters on the WLCG infrastructure.
Also, a careful pipeline design is adopted to alternate online and offline elaborations.
Specifically, the learning phase of the vectorization stage is performed once on a big set of data (see \cref{sec:vectorization} for more details) and it is freezed and re-used as is -- possibly updating it once in a while.
The clustering stage, instead, is performed online every day so to always expose the latest results to the shifters on-duty (see %Sections
\cref{sec:clustering,sec:viz,ch:opint-results}
% \ref{sec:clustering}, \ref{sec:viz} and \ref{ch:opint-results} 
% \cref{sec:clustering,sec:viz} and \cref{ch:opint-results} 
for more details).
