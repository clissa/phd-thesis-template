\chapter{Methods}
\label{chap:partI_methods}

This work tackles the problem of segmenting and counting cells in a \textit{supervised learning} framework. For this purpose, we address the segmentation task exploiting four CNN architectures belonging to the Unet and ResUnet families. 
Once the cells are detected, the final count is retrieved as the number of connected pixels in the post-processed output.
In doing so, we also test the impact of study design choices intended to reduce false negatives and promote accurate segmentation.
In addition to that, the four architectures are compared to an adaptive thresholding approach used as a baseline. 

\section{Non-ML baseline}
\label{baseline}

Machine Learning and Deep Learning have succeeded in many applications from several domains lately, thus building great expectations and becoming hot topics in current innovation processes at various societal levels.
Nevertheless, these powerful techniques are not a magic bullet to solve any data-related problem \cite{wolpert1997nofreelunch}. They come with their own challenges and limitations that are often overlooked in real-world applications, possibly causing thunderous failures due to unjustified expectations.
In fact, it is not unusual to fall victim to their popularity only to find months down the line that more straightforward methods work best for some specific task or data.

To avoid incurring in this ``ML hype curse", this work considers a simple non-ML approach as a baseline. 
In particular, an adaptive thresholding mechanism is implemented to exploit the pixel intensity information for binarization.
In practice, the input image is read as grayscale and a selection cutoff is set as a configurable quantile of the pixel intensity distribution.
A binary mask is obtained by labeling pixels above that threshold as cells, and the rest as background.
After that, the same post-processing steps as the ones adopted for the output of the ML models are applied (see \cref{sec:post_processing}).
This operation is performed for all training and validation pictures, and the goodness of fit is assessed as described in \cref{sec:model_evaluation}.
The whole procedure is then repeated varying the quantile value used for thresholding. A starting search is conducted using a coarse grid from 0.9 to 0.99 with steps of 0.01. This is intended to explore the hyperparameter space and get an idea of where the approach performs best. 
Since this happens for higher values, a finer grid from 0.97 to 0.999 with steps of 0.001 is exploited for fine-tuning.
Finally, the cutoff corresponding to the highest $F_1$ score is chosen as the optimal threshold and is later used to assess the baseline performance on the test set.

\section{Model architecture}
\label{model_architecture}

\begin{figure}
\centerline{
\includegraphics[width=0.8\textwidth]{figures/130_methods/c-resunet_architecture.pdf}
}
\caption{\textbf{Model architecture}. Each box reports an element of the entire architecture (individual descriptions in the legend). 
% The shortcut-connections along the encoding-path are supported by a 1$\times$1 convolution to enable the final sum before the max-pooling operation.
} \label{fig:model_architecture}
\end{figure}
We compare the detection and counting performance of four alternative architectures derived from two network families -- Unet and ResUnet -- commonly used for segmentation tasks.
In the former family, we pick the original Unet architecture \cite{unet} and a smaller version (small Unet) obtained by setting the initial number of filters equal to the ResUnet proposed in \citeA{deep_resunet} and scaling the following blocks consequently.
In the latter, we pick a ResUnet implementation presented in \citeA{deep_resunet} and a similar version with minor modifications.
Specifically, we add an initial 1$\times$1 convolution to simulate an RGB to grayscale conversion.
The advantage of doing so -- as opposed to apply a standard grayscale conversion -- is that the transformation is learned during training so to improve the segmentation performance.
As a further modification, we insert an additional residual block having 5$\times$5 filters -- instead of 3$\times$3 -- at the end of the encoding path. This adjustment should provide the model with a larger field of view, thus fostering a better comprehension of the context surrounding the pixel to classify.
This kind of information can be beneficial, for example, when cells clump together and pixels on their boundaries have to be segmented. 
Likewise, the analysis of some background structures (\cref{fig:dataset:bright,fig:artifacts:stripe,fig:artifacts:macaroon}) can be improved by looking at a broader context.
The resulting architecture is reported in \cref{fig:model_architecture} and it will be referred to as \textbf{cell ResUnet (c-ResUnet)} hereafter.

\section{Ablation studies}
\label{sec:ablation_studies}

Alongside the comparison of different approaches, we also tested the effect of two design choices intended to mitigate errors on challenging images containing artifacts and cells overcrowding.

% \noindent\textbf{Artifacts Oversampling (AO)}.
\subsection{Artifacts Oversampling (AO)}

The presence of biological structures or artifacts like those in \cref{fig:artifacts:macaroon,fig:artifacts:stripe}  can often fool the model into detecting false positives. 
% Indeed, their similarity with cells in terms of saturation and brightness makes it difficult for the model to handle them correctly. 
Indeed, their similarity with cells in terms of saturation and brightness hampers their correct handling.% by the model.
In addition, only a handful of examples of such structures are available, making them underrepresented in the data and complicating the task even further.
For this reason, we tried to increase the augmentation factor for these inputs to facilitate the learning process. Specifically, we selected 6 different crops representing such relevant structures and re-sampled them with the augmentation pipeline described in \cref{sec:model_training}, resulting in 150 new images for each crop.

\note[Luca][notesyellow]{riportare crop artefatti?}
% The transformations applied regarded rotations, addition of gaussian noise, elastic transformation and brightness, hue, saturation variation.

% \noindent\textbf{Weight Maps (WM)}. 
\subsection{Weight Maps (WM)} \label{sec:weights_map}

One of the toughest challenges during the inference is related to cell overcrowding. 
As a matter of fact, failing to precisely segment cells boundaries may lead to spurious connections between objects that are separated. Consequently, multiple objects are considered as a single one and the model performance deteriorates.
In order to improve cell separation, \citeA{unet} suggested leveraging a weight map that penalizes more the errors on the borders of touching cells.
Building on that, we introduce a novel implementation where single object contributions are compounded additively.
This procedure generates weights that decrease as we move away from the borders of each cell. 
At the same time, the contributions coming from single items are combined so that the global weight map presents higher values where more cells are close together (see \cref{fig:weight_calculation}).
%
% \begin{figure}
% \centerline{
%      \begin{subfigure}[]{0.4\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/130_methods/weight_calculation.eps}
%         \caption{Weight compounding}
%         \label{fig:weight_calculation}
%      \end{subfigure}
%      \begin{subfigure}[]{0.62\textwidth}
%          \centering
%          \includegraphics[ width=0.45\textwidth]{figures/130_methods/crop_mask_2451_crop.jpeg}
% \includegraphics[trim=0 0.008in 0 0, width=0.50\textwidth]{figures/130_methods/crop_weigths_2451_crop.jpeg}
%          \caption{Mask and correspondent weight map}
%          \label{fig:weight_map_example}
%      \end{subfigure}
% }
% \caption{\textbf{Weight map}. 
% \ref{fig:weight_calculation} shows the weight factors of background pixels between cells according to Eq. (\ref{weight_formula}). The dashed curves depict the weights generated by single cells as a function of the distance from their borders.
% % , respectively cell 1 on the left (blue) and cell 2 on the right (red).
% The green line illustrates the final weight obtained by adding individual contributions. 
% In \ref{fig:weight_map_example}, a target mask and the corresponding weight map.} 
% \label{fig:weight_map}
% \end{figure}
%
\begin{figure}
    \centering
    \includegraphics[width=.6\textwidth]{figures/130_methods/weight_calculation.eps}
    \caption{\textbf{Weight compounding.}
    The dashed curves depict the weights generated by single cells as a function of the distance from their borders according to \cref{eq:weight_formula}.
    The green line illustrates the final weight obtained by adding individual contributions.}
    \label{fig:weight_calculation}
\end{figure}
%
\begin{figure}
    \centering
    \subfloat[Mask]{
    \includegraphics[width=0.45\textwidth]{figures/130_methods/crop_mask_2451_crop.jpeg}
    }
    \subfloat[Weight map]{
    \includegraphics[trim=0 0.008in 0 0,
    width=0.50\textwidth]{figures/130_methods/crop_weights_2451_crop.jpeg}
    }
    \caption{\textbf{Weight map.} A target mask and the corresponding weight map}
    \label{fig:weight_map_example}
\end{figure}
%
The pseudocode\footnote{full implementation \githubweights} for a weight map is reported in Alg. \ref{algo:pseudocode_weightmap}, and an example weight map is shown in \cref{fig:weight_map_example}.

\begin{algorithm}%[H]
% \begin{algorithmic}[1]
\DontPrintSemicolon
     Initialize empty $\text{map}_j$ (mask size)\tcp*{weight map $j$-th mask}
    \For{each cell in mask} 
    {
    \tcc{loop over $i$-th cell in $j$-th mask}
         Initialize empty $\text{map}_i$  (mask size) \tcp*{weight map $i$-th cell}
        
         Add $i$-th cell to $\text{map}_i$
        
         Compute euclidean distance between each pixel of $\text{map}_i$ and the closest pixel of the $i$-th cell \label{step:distance}
        
         Compute each pixel's weight in $\text{map}_i$ according to a decreasing exponential function:
        \begin{equation}
        % \hskip 4.5cm
        \text{weight} = \exp\left\{\dfrac{-d^{2}}{2\sigma^{2}}\right\}
        \label{eq:weight_formula}
        \end{equation}
        
        \tcc {$d$ is the distance computed at step \ref{step:distance}}
        \tcc {$\sigma$ is a customizable parameter set to 25 (average cell radius)}
        
    Sum the resulting $\text{map}_i$ to the full $\text{map}_j$
    % , as illustrated in Fig. \ref{fig:weight_calculation};
}
% \end{algorithmic}
\caption{weight map pseudocode for $j$-th mask}
\label{algo:pseudocode_weightmap}
\end{algorithm}

\section{Model training}
\label{sec:model_training}
After randomly setting 70 full-size images apart as a test set, the remaining pictures were randomly split into training and validation sets. 
In particular, twelve 512x512 partially overlapping crops were extracted from each image and fed as input to the network after undergoing a standard augmentation pipeline. Common transformations were considered as rotations, addition of Gaussian noise, brightness variation and elastic transformations \cite{elastic_tranformation}. 
The crops augmentation factors were fixed differentially based on their contents. 
The 6 crops included in the artifact oversampling ablation study were re-sampled 25 times each.
Instead, all the remaining crops produced 10 augmented versions for manually segmented images and 4 for all the others.
As a result, the model was trained on a total of nearly 16000 images (70\% for training and 30\% for validation).

All competing architectures were trained from scratch under the same conditions to favour a fair comparison.
Specifically, the Adam \cite{adam} optimizer was employed with an initial learning rate of 0.006. A a scheduled decrease of 30\% was then applied if the validation loss did not improve for four consecutive epochs. 
A \textbf{weighted binary cross-entropy} loss was adopted on top of the weight maps to handle the imbalance of the two classes (weights equal to 1.5 and 1 for cells and background, respectively).
All models were trained until no improvement was observed for 20 consecutive epochs. In this way, each model was allowed to converge and the comparison was made at the best of each architecture's individual capabilities.

The approach was implemented through Keras API \cite{keras} using \texttt{TensorFlow} \cite{tensorflow} as backend. For more details, please refer to the GitHub repository\footnote{\github}.
The training was performed on 4 V100 GPUs provided by the \textit{Centro Nazionale Analisi Fotogrammi} (CNAF)\footnote{\cnaf} computing center of the \textit{National Institute for Nuclear Physics}\footnote{\infn} in Bologna.


\section{Post-processing} \label{sec:post_processing}

\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/130_methods/orig+heatmap:278.png}
        \caption{
        % Original image and raw output
        }
        \label{fig:raw_output}
        \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/130_methods/thresh+post_proc:278.png}
        \caption{
        % Thresholded and post-processed predicted masks
        }
        \label{fig:thresh+post_proc}
        \end{subfigure}
    \caption{\textbf{Model output}. 
    Top: the input image with white contours indicating annotated cells  (left) and the model's raw output  (right).
    Bottom: the predicted mask after thresholding at 0.875 (left) and the predicted mask after post-processing (right).}
    \label{fig:model_output}
\end{figure}
The final output of the model is a probability map (or heatmap), in which each pixel value represents the probability of belonging to a cell. 
% An example of this outcome is reported on the right of the \ref{fig:raw_output} if the input image on the left is provided to the model.
\Cref{fig:raw_output} reports an example of an input image (left) and the corresponding predicted heatmap (right).
% An example of this outcome is reported in the Fig. \ref{fig:raw_output} (right) if a sample input image (left) is provided to the model.
The higher the value, the higher is the confidence in classifying that pixel as signal. 
A thresholding operation was then applied on the heatmap to obtain a binary mask where groups of white connected pixels represent the detected cells. \Cref{fig:thresh+post_proc} (left) illustrates the cells detected after the binarization with different colors.
After that, ad-hoc post-processing was applied to remove isolated components of few pixels and fill the holes inside the detected cells. 
Finally, the watershed algorithm \cite{watershed} was employed with parameters set based on the average cell size.
\sidenote[Luca][notesyellow]{Descrivere meglio aggiungendo riferimenti nell'immagine}
An example of the results if provided in \cref{fig:thresh+post_proc}, where the overlapping cells in the middle present in the binary mask (left) are correctly splitted after post-processing (right). Also, the small object in the top right corner is removed
% , and the hole in the bottom right object is filled
.


\section{Model evaluation} \label{sec:model_evaluation}

% The Unet, small Unet, ResUnet and c-ResUnet architectures were evaluated and compared based on both detection and counting performance. 
All the presented approaches were evaluated and compared based on both detection and counting performance. 
Also, ablation studies were conducted to assess the impact of artifacts oversampling and weight maps.
% Also, ablation studies assessed the impact of artifacts oversampling and weight maps.

In order to evaluate the detection ability of the models, a dedicated algorithm was developed.
% Specifically, each target cell was compared to all objects in the corresponding predicted mask and uniquely associated with the closest one.
Specifically, each predicted object was compared to all cells in the corresponding ground-truth label and uniquely associated with the closest one.
If the distance between their centroids was less than a fixed threshold (50 pixels, i.e. average cell diameter), the predicted element was considered a match and it increased the true positive count (TP).
% ; a false negative otherwise (FN).
At the end of this procedure, all true objects without matches were considered as false negatives (FN). Likewise, the remaining detected items not associated with any target were considered as false positives (FP).
Algorithm \ref{algo:pseudocode_metrics} reports the pseudocode of the procedure described above\footnote{full implementation \githubmetrics}.
\begin{algorithm}%[H]
% \begin{algorithmic}[1]
    \DontPrintSemicolon
    % init
    \KwIn{ pred$_i$, mask$_i$}
    \KwOut {TP, FP, FN}       
    % \tcp*{true positives, false positives, false negatives}
    
    Set TP, FP, FN = 0
    
    Get predicted objects, pred\_objs$^i$
    \tcp*{detected cells}
    
    Get true objects, true\_objs$^i$
    \tcp*{annotated cells}
    
    % centers
    Get predicted centers, pred\_ctrs$^i$
    %  \tcp*{centers of predicted objects}
    
    Get true centers, true\_ctrs$^i$
    %  \tcp*{centers of true objects}
    
    \For{each ctr$_j$ in pred\_ctrs$^i$} 
        {
        \tcc{loop over predicted centers}
        \For{each ctr$_k$ in true\_ctrs$^i$}
            {
            \tcc{loop over true centers}
            
             Compute euclidean distance between ctr$_j$ and ctr$_k$ \label{step:ctrs_distance}
             
             Store distance and indexes
             \label{step:store_distance}
            } 
        
         Compute the minimum, min\_dist$_i$ of the distances stored in step \ref{step:store_distance}
         
        \If{understand}{
            Increase true positives, TP
            
            Remove ctr$_j$ from pred\_ctrs$^i$
            
            Remove ctr$_k$ from true\_ctrs$^i$
        }
        
        }
        
    Compute false negatives as true\_objs$^i$ - TP
    
    Compute false positives as pred\_objs$^i$ - TP
    
\caption{metrics computation for i\emph{-th} image}
\label{algo:pseudocode_metrics}
\end{algorithm}
Starting from these values, we referred to accuracy, precision, recall and $F_1$ score as indicators of detection performance.
% In terms of detection performance, the $F_1$ score was adopted as the primary indicator. Accuracy, precision and recall were also inspected to have a better understanding of the model ability. 
The definitions of such metrics are reported below:

\begin{align}
% \hskip 2cm
\text{accuracy} &=  \frac{\text{TP}}{\text{TP} + \text{FP} + \text{FN}}
= \frac{\text{1}}{\text{1} + \frac{1}{\text{TP}} \left(\text{FP} + \text{FN}\right)}
\label{eq:accuracy}; \\ 
\text{precision} &=    \frac{\text{TP}}{\text{TP} + \text{FP}}; \\
\text{recall} &=    \frac{\text{TP}}{\text{TP} + \text{FN}}; \\ 
F_1 \text{score} &=  \frac{2 * \text{precision} * \text{recall}}{\text{precision} + \text{recall}}
= \frac{2*\text{TP}}{2*\text{TP} + \text{FP} + \text{FN}} 
= \frac{\text{1}}{\text{1} + \frac{1}{\text{2TP}} \left(\text{FP} + \text{FN}\right)}
\label{eq:F1}.
\end{align}
% where TP, FP and FN indicates true positive (cells correctly detected), false positives (cells erroneously detected) and false negatives (cells erroneously missed), respectively. 
Notice that we do not have true negatives in \cref{eq:accuracy} since the prediction of the class ``not cell" is done at the pixel level and not at the object level, so there are no ``non-cell" objects predicted by the model.

Regarding the counting task, the Mean Absolute Error (MAE), Median Absolute Error (MedAE) and Mean Percentage Error (MPE) were used instead. More precisely, let $n_{\text{pred}}$ be the number of detected cells in $i$-th image  and $n_{\text{true}}$ be the actual one. Then, the absolute error (AE) and the percentage error (PE) were defined as:

\begin{align}
% \hskip 2cm
\text{AE} &= \lvert n_{\text{true}} - n_{\text{pred}}\rvert ;\\
\text{PE} &= \frac{ n_{\text{true}} - n_{\text{pred}}}{n_{\text{true}} 
% + \epsilon
}.
\end{align}
% where $\epsilon=10^{-6}$ was added to prevent a vanishing denominator when $n_{\text{true}} = 0$. 
Hence, the above counting metrics are just the mean and the median of the AE and the PE.
% The $F_1$ score was adopted for measuring the detection performance, while Mean Absolute Error (MAE), Median Absolute Error (MedAE) and Mean Percentage Error (MPE) were used as counting metrics. 

% \noindent\textbf{Threshold optimization}.
\subsection{Threshold optimization}

The choice of the optimal cutoff for binarization was based on the $F_1$ score computed on full-size images. In practice, DL models were evaluated on a grid of values and the best one was selected according to the \textit{Kneedle} method \cite{kneedle}. 
The same was done for the non-ML approach, with the only difference of considering the cutoff yielding the maximum $F_1$ value. 
The resultant thresholds were then used to assess performances on the test set.
Although the ultimate goal is retrieving the counts, we relied on detection performance to enforce accurate recognition and avoid spurious balancing between false positives and false negatives -- which are indistinguishable from the counts.
Also, full-size images (as opposed to crops) are used to simulate better the model performance in a real-world scenario.
\begin{figure}
\centerline{
\includegraphics[width=\textwidth]{figures/130_methods/F1_optimization.eps}
}
\caption{\textbf{Threshold optimization}. On the left, the $F_{1}$ score computed on validation images as a function of the cutoff for thresholding.
On the right, the test $F_1$ score of the c-ResUnet model is used to illustrate the selection of the best threshold for binarization according to \textit{argmax} (blue) and \textit{kneedle} (red) methods.
} 
\label{fig:thresh_opt}
\end{figure}
% If the distance between their centroids was less than a fixed threshold (50 pixels, i.e. average cell diameter), the predicted element was considered a true positive; a false negative otherwise.
% Detected items not associated with any target were considered as false positives instead.

\Cref{fig:thresh_opt} shows the optimization results. On the left, we can see how each model performance varies in the validation set as a function of the cutoff for binarization.
For the adaptive thresholding approach, only very high thresholds lead to acceptable performances and we observe a sharp peak followed by a rapid decrease thereafter.
% Even though lower thresholds work best for all DL models, the $F_1$ curves are rather flat after their peaks. 
On the contrary, all DL models work best for lower thresholds and present $F_1$ curves which are rather flat after their peaks.
Thus, increasing the cutoff allows focusing only on predictions whereby the model is very confident, with just a slight loss in overall performance.
Also, good practices in natural science applications suggest being conservative with counts and only consider clearly stained cells.
For these reasons, we opted for the \textit{argmax} value (0.994) for the baseline approach, while we resorted to the \textit{Kneedle} method \cite{kneedle} for the selection of the optimal DL threshold. 
An example of that choice in the case of c-ResUnet is reported in \cref{fig:thresh_opt} (right).